---
title: "Preliminaries"
author: "RJB"
date: "2025-07-17"
output: html_document
---

MCF10A seeding day -1 normal media, day 0 wash then direct onto glucose media. (because they really didn't like 0 glucose media)
All other cell lines 6 hours on starvation media, then add glucose media at 2x conc to make up to the correct concentrations.

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir="/share/lab_crd/lab_crd/HighPloidy_CostBenefits/data/GlucoseStarvation/")
```

The workflow uses image sets from various experiments which are currently saved in different locations in our shared drive. Running this script establishes symbolic links to all the images in a single shared folder. 

```{r}
## output saved to all_raw
source("scripts/mapFolders.R")
```

All metadata is contained within the filenames:
```{r}
ff <-  list.files("all_raw/")
sample(ff,10)
```


```{r}
source("scripts/define_plate_maps.R")
ff_strip <- gsub(".tif","",ff)
ff_strip <- sapply(ff_strip,function(fi){
  fi <- strsplit(fi,split="_") |> unlist()
  fi <- fi[-(length(fi)-3)]
  paste(fi,collapse="_")
}) |> unique()

meta <- do.call(rbind,pbapply::pblapply(ff_strip,get_meta))
meta$fname <- ff_strip

```

We need to define a representative set of images for evaluating our image processing pipeline. Factors to balance are 1) Cell-line 2) sub-lineage (e.g. 2N vs 4N etc), 3) experiment id (different experiments can use different reporters or other settings) 4) Time. 5) Glucose concentration. Note that we also hope our training set has many examples of both alive and dead cells, but ensuring a good sampling across 1-5 should ensure this anyway.

```{r}
set.seed(42)
Neval <- 100
## ensure equal representation per cellLine
mfilter <- meta[!meta$cellLine%in%c("MCF10A-ctrl","MCF10A-hras"),]
## ensure equal representation per cellLine
msplit <- split(mfilter,f=mfilter$cellLine)

train_data <- do.call(rbind,lapply(msplit,function(mi){
  mi[sample(1:nrow(mi),Neval/5,replace = F),]
}))
rownames(train_data) <- NULL

table(train_data$cellLine)
hist(train_data$hours)
table(train_data$glucose)
write.csv(train_data,"data/train/train_image_ids.csv",row.names = F)
```
Also make a random set of images for validation

```{r}
set.seed(1)
Neval <- 100
stop("CURRENTLY WE ARE PERFORMING VALIDATION ON AN OLDER IMAGE SET. IT CONTAINS IMAGES FROM CELL LINES NOT USED IN THIS PROJECT (NOW FILTERED OUT - SEE BELOW). IF YOU RUN THIS CODE BLOCK, YOU RISK IRRETRIEVABLY OVERWRITING THE OLD IMAGE IDS! AT SOME POINT WE WILL NEED TO COMPILE A NEW VALIDATION SET, AND MANUALLY COUNT THE CELLS AGAIN BEFORE PUBLICATION. PERHAPS IT IS EASIEST TO RECYLCE THE PREVIOUSLY COUNTED IMAGES... REGARDLESS THE OLD VALIDATION SET IS GOOD ENOUGH TO SEE WHETHER THE CLASSIFICATION PIPELINE WORKS WELL OR NOT.")
mfilter <- meta[!meta$cellLine%in%c("MCF10A-ctrl","MCF10A-hras"),]
## ensure equal representation per cellLine
msplit <- split(mfilter,f=mfilter$cellLine)

train_data <- do.call(rbind,lapply(msplit,function(mi){
  mi[sample(1:nrow(mi),Neval/5,replace = F),]
}))
rownames(train_data) <- NULL

table(train_data$cellLine)
hist(train_data$hours)
table(train_data$glucose)
write.csv(train_data,"data/eval/eval_image_ids.csv",row.names = F)
```

Clone the imutils package https://github.com/Richard-Beck/imutils. NOTE ALL ANALYSIS WAS PERFORMED WITH REPO v1.0
tatteg with "Analysis run on 2025-12-30".

Modify the scripts below (i.e. the local library load paths at the top of each) to point to the imutils dir. Execute:

Create the training set:
```{bash,eval=FALSE}
python scripts/image_processing/create_image_set.py --raw_data_dir all_raw --csv_path data/train/train_image_ids.csv --composites_dir data/train/composites --cpose_inputs_dir data/train/cellpose_inputs

```

Create the validation set:
```{bash,eval=FALSE}
python scripts/image_processing/create_image_set.py --raw_data_dir all_raw --csv_path data/eval/eval_image_ids.csv --composites_dir data/eval/composites --cpose_inputs_dir data/eval/cellpose_inputs

```

TRAIN CLASSIFIER:
```{bash,eval=FALSE}
python scripts/image_processing/train_classifier_interactive.py --images_dir data/train/composites --output_dir data/train/classifier_training_outputs --cpose_inputs_dir data/train/cellpose_inputs
```

RUN FULL PIPELINE ON TRAIN/EVAL SETS:
```{bash,eval=FALSE}
python scripts/image_processing/quick_segment_and_classify.py --composite_dir data/train/composites --cpose_input_dir data/train/cellpose_inputs --output_csv_path data/train/curation_outputs/training_predictions.csv --classifier_path data/train/curation_outputs/object_classifier.pkl
```

```{bash,eval=FALSE}
python scripts/image_processing/quick_segment_and_classify.py --composite_dir data/eval/composites --cpose_input_dir data/eval/cellpose_inputs --output_csv_path data/eval/object_predictions.csv --classifier_path data/train/classifier_training_outputs/object_classifier.pkl --output_masks_dir data/eval/masks
```

RUN TRAINED PIPELINE ON ALL DATA (BATCH)

```{bash,eval=FALSE}
python scripts/image_processing/batch_segment_and_classify.py  --raw_data_dir all_raw --classifier_path data/train/curation_outputs/object_classifier.pkl --output_csv_path data/counts/batch_results.csv
```


The next few chunks are for validation of cell counting pipeline, but they may be outdated - to do is to check each of these and either deleted or keep as necessary

```{r}
library(dplyr)
library(data.table)

# Load Old (Prior) and New (Forensic) results
old_res <- fread("data/eval/untuned_object_predictions.csv")
new_res <- fread("data/eval/forensic_check.csv")

# 1. Check Row Counts
cat("Old rows:", nrow(old_res), "\nNew rows:", nrow(new_res), "\n")

# 2. Check Predictions for matching Objects
# We join by image_key and object_id. 
# NOTE: Object IDs might shift if segmentation is slightly different.
# If IDs shift, we have a segmentation mismatch (different model/version).
comparison <- inner_join(
  old_res %>% select(image_key, object_id, old_pred = predicted_label_name),
  new_res %>% select(image_key, object_id, new_pred = predicted_label_name),
  by = c("image_key", "object_id")
)

# Calculate agreement
agreement <- mean(comparison$old_pred == comparison$new_pred)
cat(sprintf("Agreement on identical Object IDs: %.2f%%\n", agreement * 100))

# 3. Check Total Counts (More robust to slight ID shifts)
old_counts <- old_res %>% count(image_key, predicted_label_name)
new_counts <- new_res %>% count(image_key, predicted_label_name)

full_join(old_counts, new_counts, by=c("image_key", "predicted_label_name")) %>%
  mutate(diff = n.x - n.y) %>%
  filter(diff != 0) %>%
  print()
```

    
```{r}
library(dplyr)
library(tidyr)
library(jsonlite)

# Load and process manual point labels
manual_df <- fromJSON("data/eval_old/labelling_output_points/session_point_labels.json") %>%
  filter(label %in% c("alive", "dead")) %>%
  count(image, label) %>%
  pivot_wider(
    names_from = label,
    values_from = n,
    names_glue = "{label}_manual",
    values_fill = 0
  )

# Load and process batch classifier predictions
tmp <- data.table::fread("data/eval/object_predictions.csv")
pred_df <- do.call(rbind,lapply(unique(tmp$image_key),function(ti){
  alive_pred=sum(tmp$predicted_label_name[tmp$image_key==ti]=="alive")
  dead_pred=sum(tmp$predicted_label_name[tmp$image_key==ti]=="dead")
  data.table(image=ti,alive_pred,dead_pred)
}))



# Join the two datasets
comparison_df <- full_join(manual_df, pred_df, by = "image") %>%
  mutate(across(where(is.numeric), ~ replace_na(., 0))) %>%
  select(image, ends_with("_manual"), ends_with("_pred"))

meta <- do.call(rbind,pbapply::pblapply(comparison_df$image,get_meta))

df <- cbind(comparison_df,meta)

df$total_manual <- df$alive_manual+df$dead_manual
df$total_pred <- df$alive_pred+df$dead_pred

p <- ggplot(df)+
  facet_wrap(~cellLine,scales="free")+
  geom_abline()+
  geom_point(aes(x=alive_manual,y=alive_pred,color="alive"))+
  geom_point(aes(x=dead_manual,y=dead_pred,color="dead"))+
  geom_point(aes(x=total_manual,y=total_pred,color="total"))+
  scale_x_continuous("manual counts")+
  scale_y_continuous("predicted counts")
p

cor(df$alive_manual,df$alive_pred)
cor(df$dead_manual,df$dead_pred)
cor(df$total_manual,df$total_pred)



```

```{r}
# Prerequisites:
# install.packages(c("magick", "tiff", "jsonlite", "tidyverse"))
# BiocManager::install("EBImage")

library(EBImage)
library(jsonlite)
library(tidyverse)
library(magick) # Use this for Image Loading
library(tiff)   # Use this for Mask Loading

# --- Configuration ---
DIRS <- list(
  composites = "data/eval/composites",
  cpose_inputs = "data/eval/cellpose_inputs",
  masks = "data/eval/masks",
  output = "data/eval/visualization_panels"
)

FILES <- list(
  predictions = "data/eval/object_predictions.csv",
  manual_gt = "data/eval_old/labelling_output_points/session_point_labels.json"
)

dir.create(DIRS$output, recursive = TRUE, showWarnings = FALSE)

# --- Helper Functions ---

get_base_key <- function(filename) {
  name <- tools::file_path_sans_ext(basename(filename))
  name <- gsub("_cpose_input$", "", name)
  name <- gsub("_composite$", "", name)
  name
}

# Wrapper to load any image via Magick -> EBImage
load_image_robust <- function(path) {
  # 1. Read with Magick (handles 64-bit float)
  m_img <- image_read(path)
  
  # 2. Convert to EBImage object
  eb_img <- magick::as_EBImage(m_img)
  
  # 3. Normalize to 0-1 range for consistent display brightness
  # (Fixes issues if data is 0.04-0.05 range)
  return(normalize(eb_img))
}

# --- Load Data ---

cat("Loading data tables...\n")
preds_df <- read_csv(FILES$predictions, show_col_types = FALSE)

manual_df <- tryCatch({
  df <- fromJSON(FILES$manual_gt)
  df$image_key <- sapply(df$image, get_base_key)
  df
}, error = function(e) {
  warning("Skipping GT panel (JSON not found).")
  NULL
})

image_keys <- unique(preds_df$image_key)

# --- Processing Loop ---

for (key in image_keys) {
  cat(sprintf("Processing: %s\n", key))
  
  path_comp <- file.path(DIRS$composites, paste0(key, "_composite.tif"))
  path_cpose <- file.path(DIRS$cpose_inputs, paste0(key, "_cpose_input.tif"))
  path_mask <- file.path(DIRS$masks, paste0(key, "_mask.tif"))
  
  if (!file.exists(path_comp) || !file.exists(path_mask)) next

  # 1. Load Images (Magick -> EBImage)
  # comp_img is [W, H, 3] (Dead, Alive, Phase)
  comp_img <- load_image_robust(path_comp)
  
  # cpose_img is [W, H, 2] (Phase, FluorSum) 
  # OR [W, H, 1] if single channel.
  cpose_img <- load_image_robust(path_cpose)
  
  # 2. Load Mask (TIFF -> Matrix)
  # t() Transpose is CRITICAL because readTIFF is [Y, X] and EBImage is [X, Y]
  mask_mat <- t(readTIFF(path_mask, as.is = TRUE))

  # 3. Extract Slices (Treat as simple arrays)
  # Python Saved Composite: [Dead, Alive, Phase] -> R Indices: 1, 2, 3
  ch_dead  <- imageData(comp_img)[,,1]
  ch_alive <- imageData(comp_img)[,,2]
  ch_phase <- imageData(comp_img)[,,3]
  
  # Python Saved Cpose Input: [Phase, FluorSum] -> R Indices: 1, 2
  cpose_phase <- imageData(cpose_img)[,,1]
  cpose_fluor <- if (dim(cpose_img)[3] >= 2) imageData(cpose_img)[,,2] else array(0, dim=dim(cpose_phase))

  # --- Panel A: Composite (ImageJ Style) ---
  # Red   = Phase + Dead
  # Green = Phase + Alive
  # Blue  = Phase
  # pmin(..., 1) clips to 1.0 to prevent display rollover
  R <- pmin(ch_phase + ch_dead, 1)
  G <- pmin(ch_phase + ch_alive, 1)
  B <- ch_phase
  
  # Construct RGB Image
  panel_composite <- rgbImage(red = R, green = G, blue = B)

  # --- Panel B: Segmentation ---
  # Background: Phase (Gray) + FluorSum (Green Tint)
  seg_base <- rgbImage(
    red   = ch_phase, 
    green = ch_phase, 
    blue  = pmin(ch_phase + cpose_fluor, 1)
  )
  # Paint Outlines (Yellow)
  panel_seg <- paintObjects(mask_mat, seg_base, col = c("#FFFF00", NA), thick = TRUE, opac = c(1, 0))

  # --- Panel C: Classification ---
  key_preds <- preds_df %>% filter(image_key == key)
  
  ids_alive <- key_preds %>% filter(predicted_label_name == "alive") %>% pull(object_id)
  ids_dead  <- key_preds %>% filter(predicted_label_name == "dead") %>% pull(object_id)
  
  mask_alive <- mask_mat; mask_alive[!mask_alive %in% ids_alive] <- 0
  mask_dead  <- mask_mat; mask_dead[!mask_dead %in% ids_dead]   <- 0
  
  # Base: Grayscale Phase
  class_base <- toRGB(Image(ch_phase))
  
  # Paint Dead (Red)
  res <- paintObjects(mask_dead, class_base, col = c("red", "red"), opac = c(1, 0.3), thick = TRUE)
  # Paint Alive (Green)
  panel_class <- paintObjects(mask_alive, res, col = c("green", "green"), opac = c(1, 0.3), thick = TRUE)

  # --- Panel D: Ground Truth ---
  panel_gt <- toRGB(Image(ch_phase))
  if (!is.null(manual_df)) {
    pts <- manual_df %>% filter(image_key == key)
    if (nrow(pts) > 0) {
      for (i in 1:nrow(pts)) {
        col_pt <- if(pts$label[i] == "alive") "green" else "red"
        try({
          # drawCircle x/y matches the [X, Y] layout of our images
          panel_gt <- drawCircle(panel_gt, x = pts$x[i], y = pts$y[i], r = 6, col = col_pt, fill = TRUE)
        }, silent=TRUE)
      }
    }
  }

  # --- Combine and Save ---
  # Explicitly use EBImage::combine to avoid conflicts
  combined <- EBImage::combine(panel_composite, panel_seg, panel_class, panel_gt)
  
  # Tile 4x1 (Horizontal Strip)
  final_strip <- tile(combined, nx = 4, lwd = 2, fg.col="white")
  
  out_path <- file.path(DIRS$output, paste0(key, "_panel.png"))
  writeImage(final_strip, out_path)
}

cat("Done!\n")
```



```{r}
source("scripts/define_plate_maps.R")
library(ggplot2)
x <- data.table::fread("data/counts/batch_results_v2.csv")
x <- x[, time := substr(image_key, nchar(image_key) - 8, nchar(image_key) )]
x <- x[, base_key := substr(image_key, 1, nchar(image_key) - 10)]
x <- split(x,f=x$base_key)

meta <- do.call(rbind,lapply(paste0(names(x),"_00d00h00m"),get_meta))

meta <- data.table(meta)

# add a base_key column so meta and x line up
meta[, base_key := names(x)]

# build a grouping key
meta[, grp := paste(cellLine, experiment, plateID, ploidy, glucose, sep="|")]

# split the base_keys by grp
groups <- split(meta$base_key, meta$grp)

# for each group, rbindlist the corresponding xâ€™s
x_merged <- lapply(groups, function(bk) 
  rbindlist(x[bk], use.names=TRUE, fill=TRUE)
)

names(x_merged) <- names(groups)


```


```{r}
smm <- do.call(rbind,lapply(names(x_merged), function(grp) {
  dtg <- x_merged[[grp]]
  # count per well & time string
  cnt <- dtg[, .(
    alive = sum(predicted_label_name=="alive", na.rm=TRUE),
    dead  = sum(predicted_label_name=="dead",  na.rm=TRUE)
  ), by=.(time)]
  m <- str_match(cnt$time, "(\\d{2})d(\\d{2})h")
  cnt[, hours := as.integer(m[,2])*24 + as.integer(m[,3])]
  flds <- str_split(grp, "\\|")[[1]]
  cnt[, c("cellLine","experiment","plateID","ploidy","glucose") := 
        list(flds[1],flds[2],flds[3],flds[4],flds[5])]
  cnt}))

saveRDS(smm,"data/counts/uncorrected.Rds")

plots <- lapply(unique(smm$cellLine), function(cl) {
  d <- smm[cellLine == cl]
  ggplot(d, aes(x = hours)) +
    geom_point(aes(y = alive, color = "alive")) +
    geom_point(aes(y = dead, color = "dead")) +
    facet_grid(rows = vars(as.numeric(glucose)),
               cols = vars(ploidy),
               scales = "free") +
    ggtitle(cl, subtitle = unique(d$experiment))
})
names(plots) <- unique(smm$cellLine)

# Example: display the first plot
plots

```

Glucose data calibration. There's something "amiss" with three of the data points in the calibration assay (Didem appears to have excluded three points). Excluding them does result in greater confidence in glucose prediction based on the fluorescence - but is that confidence misplaced? More investigation is warranted.

N.B everything below this is entirely deprecated, but remains because it is reasonably illustrative (maybe useful for generating plots later). All logic for glucose data handling and data integration is now in MCMC/prepare_data.R


```{r}

costf <- function(pars,x){
  a <- pars[1]
  b <- pars[2]
  pars <- abs(pars)
  lval <- log(x$Lum)
  lpred <- log(x$G*a+b)
  err <- sum((lval-lpred)^2)
  if(is.na(err)) return(10^12)
  return(err)
}

glu_prob <- function(glu,fluor,a,b,sdest){
  Efluor <- a*glu + b
  dnorm(log(fluor)-log(Efluor),sd=sdest)
}

predf <- function(pars,x=x){
    a <- pars[1]
    b <- pars[2]
    pars <- abs(pars)
    x$G*a+b
}

x <- data.table::fread("data/glucose/processed/SUM-159-chem.SNU668/calibration.csv")
x <- data.table::fread("data/glucose/processed/MCF10A.MDA-MB-231/calibration.csv")
xsd <- aggregate(list(sd=x$Lum),by=list(G=x$G),sd)


fit0 <- lm(Lum~G,data=x)
par0 <- c(log(min(x$Lum)),as.numeric(coef(fit0)["G"]))
fit <- optim(par0,costf,x=x)

x$pLum <- predf(fit$par,x=x)
x$pLumLin <- predict(fit0)

sdlog <- sd(log(x$pLum)-log(x$Lum))

## heteroscedacity 
p <- ggplot(xsd,aes(x=G,y=sd))+
  geom_point()
p


p <- ggplot(x,aes(x=G,y=Lum))+
  geom_point()+
  geom_line(aes(y=pLum,color="multiplicative"))+
  geom_line(aes(y=pLumLin,color="linear"))+
  scale_x_log10()+
  scale_y_log10()
p

xmpl <- data.frame(G=exp(seq(-15,0,by=0.1)),lum=7000)
xmpl$p <- glu_prob(xmpl$G,xmpl$lum,a=fit$par[2],b=fit$par[1],sdest=sdlog)

p <- ggplot(xmpl,aes(x=G,y=p))+
  geom_line()
p


```


Matching glucose and cell line data... and bundling into a nice fitting object.

The glu_map tells which glucose assay data corresponds to which cellcounts data. It is intended that
for each cellLine x experiment combo, there should be a corresponding folder containing the glucose assay data...

```{r}
glu_map <- c('MCF10A-ctrl.A00-IncucyteRawDataLiveDead-varyGlucose-241015'=NULL,
             'MCF10A-hras.A00-IncucyteRawDataLiveDead-varyGlucose-241015'=NULL,
             'MCF10A.A00b-IncucyteRawDataLiveDead-varyGlucose-241015'    ="MCF10A.MDA-MB-231",
             'MCF10A.A00c-IncucyteRawDataLiveDead-varyGlucose-241015'    ="MCF10A.MDA-MB-231",
             'MDA-MB-231.B00-IncucyteRawDataLiveDead-varyGlucose-250213' ="MCF10A.MDA-MB-231",
             'SNU668.A00-IncucyteRawDataLiveDead-varyGlucose-250324'     ="SUM-159-chem.SNU668",
             'SUM-159-chem.M00b-IncucyteRawDataLiveDead-varyGlucose' = "SUM-159-chem.SNU668",   
             'SUM-159-fuse.C00-IncucyteRawDataLiveDead-varyGlucose'      =NULL,
             'SUM-159-fuse.I00-IncucyteRawDataLiveDead-varyGlucose' ="SUM-159-fuse")
```


```{r}
ploidy_map <- list('SUM-159-chem'=c("2N"="low","4N"="high"),
                   SNU668=c(high="high",low="low"),
                   MCF10A=c("4N"="high","2N"="low"),
                   'MDA-MB-231'=c(parental="high","3N"="low"),
                   'SUM-159-fuse'=c("2N"="low","4N"="high"))
```


```{r}

costf <- function(pars,x){
  a <- pars[1]
  b <- pars[2]
  pars <- abs(pars)
  lval <- log(x$Lum)
  lpred <- log(x$G*a+b)
  err <- sum((lval-lpred)^2)
  if(is.na(err)) return(10^12)
  return(err)
}

predf <- function(pars,x){
    a <- pars[1]
    b <- pars[2]
    pars <- abs(pars)
    x$G*a+b
}

make_glucose_model <- function(xg, pars){
  a <- unname(pars["a"]); b <- unname(pars["b"]); sdlog <- unname(pars["sdlog"])
  stopifnot(is.finite(a), is.finite(b), is.finite(sdlog), sdlog > 0)
  get_pLum <- function(G, dat){
    dilution <- 1000/unique(dat$`Dilution Factor`)
    mu <- a*G*dilution + b                       # expected fluorescence at the provided G
    p  <- dnorm(log(dat$lum) - log(mu), sd = sdlog)  # P(obs lum | G)
    return(p)
  }

  return(get_pLum)
}


process_glucose_assay <- function(path2data){
  x <- data.table::fread(file.path(path2data,"calibration.csv"))
  fit0 <- lm(Lum~G,data=x)
  par0 <- c(log(min(x$Lum)),as.numeric(coef(fit0)["G"]))
  fit <- optim(par0,costf,x=x)
  pars <- fit$par
  names(pars) <- c("a","b")
  x$pLum <- predf(pars,x=x)
  sdlog <- sd(log(x$pLum)-log(x$Lum))
  pars <- c(pars,sdlog=sdlog)
  
  xg <- data.table::fread(file.path(path2data,"data.csv"))
  tmp <- xg[is.na(xg$ploidy),]
  xg$ploidy[is.na(xg$ploidy)] <- "low"
  tmp$ploidy[is.na(tmp$ploidy)] <- "high"
  xg <- rbind(xg,tmp)  
  xg <- reshape2::melt(xg,measure.vars = paste0("R",1:3))
  xg$hours <- xg$Day*24
  xg <- xg[,c("CellLine","ploidy", "G0", "hours", "value","Dilution Factor")]
  colnames(xg)[colnames(xg)=="value"] <- "lum"
  colnames(xg)[colnames(xg)=="CellLine"] <- "cellLine"
  
  list(glucose=xg,ll_lum=make_glucose_model(xg, pars))
}

ff <- list.files("data/glucose/processed",full.names = T)

for(path2data in ff){
  x <- process_glucose_assay(path2data)
  saveRDS(x,file.path(path2data,"fit_obj.Rds"))
  test_dat <- x$glucose%>%filter(cellLine==x$glucose$cellLine[1],hours==0,G0==5)
  test_gluc <- seq(0,10,0.1)
  test_prob <- sapply(test_gluc,function(gi){
    prod(x$ll_lum(gi,test_dat))
  })
  
  plot(test_gluc,test_prob)
}






```


