---
title: "Preliminaries"
author: "RJB"
date: "2025-07-17"
output: html_document
---

MCF10A seeding day -1 normal media, day 0 wash then direct onto glucose media. (because they really didn't like 0 glucose media)
All other cell lines 6 hours on starvation media, then add glucose media at 2x conc to make up to the correct concentrations.

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir="/share/lab_crd/lab_crd/HighPloidy_CostBenefits/data/GlucoseStarvation/")
```

The workflow uses image sets from various experiments which are currently saved in different locations in our shared drive. Running this script establishes symbolic links to all the images in a single shared folder. 

```{r}
## output saved to all_raw
source("scripts/mapFolders.R")
```

All metadata is contained within the filenames:
```{r}
ff <-  list.files("all_raw/")
sample(ff,10)
```


```{r}
source("scripts/define_plate_maps.R")
ff_strip <- gsub(".tif","",ff)
ff_strip <- sapply(ff_strip,function(fi){
  fi <- strsplit(fi,split="_") |> unlist()
  fi <- fi[-(length(fi)-3)]
  paste(fi,collapse="_")
}) |> unique()

meta <- do.call(rbind,pbapply::pblapply(ff_strip,get_meta))
meta$fname <- ff_strip

```

We need to define a representative set of images for evaluating our image processing pipeline. Factors to balance are 1) Cell-line 2) sub-lineage (e.g. 2N vs 4N etc), 3) experiment id (different experiments can use different reporters or other settings) 4) Time. 5) Glucose concentration. Note that we also hope our training set has many examples of both alive and dead cells, but ensuring a good sampling across 1-5 should ensure this anyway.

```{r}
set.seed(42)
Neval <- 100
## ensure equal representation per cellLine
msplit <- split(meta,f=meta$cellLine)

train_data <- do.call(rbind,lapply(msplit,function(mi){
  mi[sample(1:nrow(mi),Neval/4,replace = F),]
}))
rownames(train_data) <- NULL

table(train_data$cellLine)
hist(train_data$hours)
table(train_data$glucose)
write.csv(train_data,"data/train/train_image_ids.csv",row.names = F)
```
Also make a random set of images for validation

```{r}
set.seed(1)
Neval <- 100
## ensure equal representation per cellLine
msplit <- split(meta,f=meta$cellLine)

train_data <- do.call(rbind,lapply(msplit,function(mi){
  mi[sample(1:nrow(mi),Neval/4,replace = F),]
}))
rownames(train_data) <- NULL

table(train_data$cellLine)
hist(train_data$hours)
table(train_data$glucose)
write.csv(train_data,"data/eval/eval_image_ids.csv",row.names = F)
```

Clone the imutils package https://github.com/Richard-Beck/imutils.
Modify the scripts below (at the top) to point to the imutils dir. Execute e.g:

```{bash,eval=FALSE}
python scripts/image_processing/create_image_set.py --raw_data_dir all_raw --csv_path data/train/train_image_ids.csv --composites_dir data/train/composites --cpose_inputs_dir data/train/cellpose_inputs

```

```{bash,eval=FALSE}
python scripts/image_processing/create_image_set.py --raw_data_dir all_raw --csv_path data/eval/eval_image_ids.csv --composites_dir data/eval/composites --cpose_inputs_dir data/eval/cellpose_inputs

python scripts/image_processing/create_image_set.py --raw_data_dir all_raw --csv_path data/eval/eval_image_ids.csv --composites_dir data/eval/composites2 --cpose_inputs_dir data/eval/cellpose_inputs2 --multichannel_dir data/eval/multichannel

```

Now need to finetune cellpose. Run e.g.:

```{bash,eval=FALSE}
python scripts/image_processing/curate_masks.py --raw_data_dir data/train/cellpose_inputs --output_dir data/train/curation_outputs --display_data_dir data/train/composites
```

MISSING - NEED TO PROVIDE SCRIPT FOR RUNNING FINETUNED CELLPOSE ON THE cellpose_inputs in order to train classifier on cellpose output

TRAIN CLASSIFIER:
```{bash,eval=FALSE}
python scripts/image_processing/train_classifier_interactive.py --images_dir data/train/composites --output_dir data/train/curation_outputs --masks_dir data/train/tuned-cpsam-masks
```

RUN FULL PIPELINE ON TRAIN/EVAL SETS:
```{bash,eval=FALSE}
python scripts/image_processing/quick_segment_and_classify.py --composite_dir data/train/composites --cpose_input_dir data/train/cellpose_inputs --output_csv_path data/train/curation_outputs/training_predictions.csv --classifier_path data/train/curation_outputs/object_classifier.pkl --finetuned_cellpose_model data/train/models/cpsam-tuned
```

```{bash,eval=FALSE}
python scripts/image_processing/quick_segment_and_classify.py --composite_dir data/eval/composites --cpose_input_dir data/eval/cellpose_inputs --output_csv_path data/eval/object_predictions.csv --classifier_path data/train/curation_outputs/object_classifier.pkl --finetuned_cellpose_model data/train/models/cpsam-tuned
```

DEMO RUN TRAINED PIPELINE ON ALL DATA (BATCH)

```{bash,eval=FALSE}
python scripts/image_processing/batch_segment_and_classify.py  --raw_data_dir all_raw --classifier_path data/train/curation_outputs/object_classifier.pkl --finetuned_cellpose_model data/train/models/cpsam-tuned --output_csv_path data/counts/batch_results.csv
```


DEMO SCRIPT GENERATING POINT LABELS (TRAIN AND EVAL!)
MOVE FILES ABOUT TO MATCH SCRIPTS, DELETE OLDIES!
DELETE OLD PYTHON SCRIPTS
GIT!!

The next few chunks are for validation of cell counting pipeline, but they may be outdated - to do is to check each of these and either deleted or keep as necessary
    
```{r}
library(dplyr)
library(tidyr)
library(jsonlite)

# Load and process manual point labels
manual_df <- fromJSON("data/eval/labelling_output_points/session_point_labels.json") %>%
  filter(label %in% c("alive", "dead")) %>%
  count(image, label) %>%
  pivot_wider(
    names_from = label,
    values_from = n,
    names_glue = "{label}_manual",
    values_fill = 0
  )

# Load and process batch classifier predictions
tmp <- data.table::fread("data/eval/object_predictions.csv")
pred_df <- do.call(rbind,lapply(unique(tmp$image_key),function(ti){
  alive_pred=sum(tmp$predicted_label_name[tmp$image_key==ti]=="alive")
  dead_pred=sum(tmp$predicted_label_name[tmp$image_key==ti]=="dead")
  data.table(image=ti,alive_pred,dead_pred)
}))

# Join the two datasets
comparison_df <- full_join(manual_df, pred_df, by = "image") %>%
  mutate(across(where(is.numeric), ~ replace_na(., 0))) %>%
  select(image, ends_with("_manual"), ends_with("_pred"))

meta <- do.call(rbind,pbapply::pblapply(comparison_df$image,get_meta))

df <- cbind(comparison_df,meta)

p <- ggplot(df)+
  facet_wrap(~cellLine,scales="free")+
  geom_abline()+
  geom_point(aes(x=alive_manual,y=alive_pred,color="alive"))+
  geom_point(aes(x=dead_manual,y=dead_pred,color="dead"))+
  scale_x_continuous("manual counts")+
  scale_y_continuous("predicted counts")
p

cor(df$alive_manual,df$alive_pred)
cor(df$dead_manual,df$dead_pred)
```


Python eval_training_set.py si run to generate manually curated masks and predictions. The results are analyzed as follows:
```{r}
# Object Count Summarizer & Plotter
#
# This script counts objects in curated and predicted masks, then creates a
# scatter plot to compare the counts for each image, faceted by cell line.


# --- 1. Required Libraries ---
# Ensure you have these libraries installed.
# install.packages(c("tiff", "dplyr", "purrr", "stringr", "tidyr", "ggplot2"))

library(tiff)
library(dplyr)
library(purrr)
library(stringr)
library(tidyr)
library(ggplot2)

# --- 2. Configuration ---
# Define the paths to your mask directories.
PREDICTED_MASKS_DIR <- "data/train/tuned-cpsam-masks"
GROUND_TRUTH_DIR    <- "data/train/curated-masks"
OUTPUT_PLOT_PATH    <- "object_count_comparison.png"

# --- 3. Helper Function: Count Objects and Extract Metadata ---
get_counts_from_file <- function(file_path) {
  
  # Read the mask file
  mask <- readTIFF(file_path)
  
  # Count unique non-zero labels to get the number of objects
  object_count <- length(setdiff(unique(as.vector(mask)), 0))
  
  # Create a unique key from the filename to join on
  file_name <- basename(file_path)
  image_key <- str_remove(file_name, "_cpsam_mask.tif|_curated_mask.tif|_cpose_input_mask.tif")
  
  # Extract the cell line from the filename
  cell_line <- str_split(file_name, "_", simplify = TRUE)[1, 1]
  
  # Return a single-row data frame (tibble)
  tibble(
    image_key = image_key,
    cell_line = cell_line,
    object_count = object_count
  )
}

# --- 4. Main Execution ---

# Get lists of all mask files
predicted_files <- list.files(PREDICTED_MASKS_DIR, pattern = "\\.tif$", full.names = TRUE)
gt_files <- list.files(GROUND_TRUTH_DIR, pattern = "\\.tif$", full.names = TRUE)

if (length(predicted_files) == 0 || length(gt_files) == 0) {
  stop("Mask files not found in one or both specified directories.")
}

# Process both sets of files
cpose_counts <- map_dfr(predicted_files, get_counts_from_file)
manual_counts <- map_dfr(gt_files, get_counts_from_file)

# Join the dataframes to create a comparison table for plotting
comparison_df <- inner_join(
  cpose_counts,
  manual_counts,
  by = c("image_key", "cell_line"),
  suffix = c("_cpose", "_manual")
)

# --- 5. Generate and Save Plot ---

# Create the scatter plot
count_plot <- ggplot(comparison_df, aes(x = object_count_cpose, y = object_count_manual)) +
  geom_point(alpha = 0.6, size = 2) +
  # Add a y=x line for reference (perfect agreement)
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  # Create a separate plot for each cell line
  facet_wrap(~ cell_line, scales = "free") +
  # Add labels and a title
  labs(
    title = "Comparison of Object Counts: Manual vs. Cellpose",
    subtitle = "Points on the red line indicate perfect agreement",
    x = "Object Count (Cellpose)",
    y = "Object Count (Manual)"
  ) +
  # Use a clean theme
  theme_bw()+
  scale_x_sqrt()+
  scale_y_sqrt()

count_plot
```


```{r}
# Segmentation Accuracy Metrics Calculator
#
# This script compares predicted segmentation masks (from a model like Cellpose)
# against manually curated ground truth masks to calculate performance metrics.
# VERSION 2: Includes a more robust one-to-one matching algorithm.

# --- 1. Required Libraries ---
# Ensure you have these libraries installed.
# install.packages(c("tiff", "dplyr", "purrr", "stringr"))

library(tiff)
library(dplyr)
library(purrr)
library(stringr)

# --- 2. Configuration ---
# Define the paths to your mask directories.
# Please update these paths to match your project structure.
PREDICTED_MASKS_DIR <- "data/train/cpsam-masks"
GROUND_TRUTH_DIR    <- "data/train/curated-masks"
IOU_THRESHOLD       <- 0.5 # Threshold to consider a prediction a "match"

# --- 3. Helper Function: Calculate IoU Matrix ---
calculate_iou_matrix <- function(pred_mask, gt_mask) {
  pred_labels <- setdiff(unique(as.vector(pred_mask)), 0)
  gt_labels <- setdiff(unique(as.vector(gt_mask)), 0)
  
  if (length(pred_labels) == 0 || length(gt_labels) == 0) {
    # Return a matrix with correct dimensions even if one is empty
    return(matrix(0, 
                  nrow = length(gt_labels), 
                  ncol = length(pred_labels),
                  dimnames = list(gt_labels, pred_labels)))
  }
  
  iou_matrix <- matrix(0, 
                       nrow = length(gt_labels), 
                       ncol = length(pred_labels),
                       dimnames = list(gt_labels, pred_labels))
  
  for (gt_id in gt_labels) {
    gt_binary <- gt_mask == gt_id
    for (pred_id in pred_labels) {
      pred_binary <- pred_mask == pred_id
      intersection <- sum(gt_binary & pred_binary)
      union <- sum(gt_binary | pred_binary)
      if (union > 0) {
        iou_matrix[as.character(gt_id), as.character(pred_id)] <- intersection / union
      }
    }
  }
  
  return(iou_matrix)
}

# --- 4. Main Function: Analyze a Single Image Pair (REVISED) ---
analyze_pair <- function(pred_path, gt_path, iou_threshold) {
  pred_mask <- readTIFF(pred_path)
  gt_mask <- readTIFF(gt_path)
  
  iou_matrix <- calculate_iou_matrix(pred_mask, gt_mask)
  
  num_gt_objects <- nrow(iou_matrix)
  num_pred_objects <- ncol(iou_matrix)
  
  if (num_gt_objects == 0 & num_pred_objects == 0) {
    return(list(tp = 0, fp = 0, fn = 0, splits = 0, merges = 0, matched_ious = numeric(0)))
  }
  
  # Convert matrix to a long-format data frame of potential matches
  potential_matches <- as.data.frame(as.table(iou_matrix)) %>%
    `colnames<-`(c("gt_id", "pred_id", "iou")) %>%
    filter(iou > 0) # Keep all non-zero overlaps for analysis

  # --- Independent Split/Merge Calculation (Diagnostic) ---
  # This calculation happens on all potential connections before filtering by threshold.
  # A split is when one GT object is plausibly covered by multiple predicted objects.
  splits <- potential_matches %>%
    group_by(gt_id) %>%
    summarise(n = n(), .groups = 'drop') %>%
    filter(n > 1) %>%
    nrow()

  # A merge is when one prediction covers multiple GT objects.
  merges <- potential_matches %>%
    group_by(pred_id) %>%
    summarise(n = n(), .groups = 'drop') %>%
    filter(n > 1) %>%
    nrow()

  # --- Robust One-to-One Matching for TP/FP/FN ---
  # Filter matches by the IoU threshold for the main calculation
  matches_over_thresh <- potential_matches %>% filter(iou >= iou_threshold)
  
  matched_pairs <- data.frame(gt_id=character(), pred_id=character(), iou=numeric())

  # Iteratively find the best match and remove participants from consideration
  while(nrow(matches_over_thresh) > 0) {
    # Find the best remaining pair
    best_pair <- matches_over_thresh %>%
      slice_max(order_by = iou, n = 1, with_ties = FALSE)
    
    # Add it to our list of true positives
    matched_pairs <- rbind(matched_pairs, best_pair)
    
    # Remove the matched GT and Pred objects from the pool
    matches_over_thresh <- matches_over_thresh %>%
      filter(gt_id != best_pair$gt_id, pred_id != best_pair$pred_id)
  }

  tp <- nrow(matched_pairs)
  fp <- num_pred_objects - tp
  fn <- num_gt_objects - tp
  
  return(list(
    tp = tp,
    fp = fp,
    fn = fn,
    splits = splits,
    merges = merges,
    matched_ious = matched_pairs$iou
  ))
}

# --- 5. Main Execution ---

pred_files <- list.files(PREDICTED_MASKS_DIR, pattern = "\\.tif$", full.names = TRUE)

if (length(pred_files) == 0) {
  stop("No predicted masks found in the specified directory.")
}

all_results <- map(pred_files, function(pred_path) {
  file_name <- basename(pred_path)
  gt_path <- file.path(GROUND_TRUTH_DIR, str_replace(file_name, "_cpsam_mask", "_curated_mask"))
  
  if (!file.exists(gt_path)) {
    warning(paste("Missing ground truth for:", file_name))
    return(NULL)
  }
  
  cat("Processing:", file_name, "\n")
  tryCatch({
    analyze_pair(pred_path, gt_path, IOU_THRESHOLD)
  }, error = function(e) {
    warning(paste("Error processing", file_name, ":", e$message))
    return(NULL)
  })
}) %>%
  compact()

# Aggregate results
total_tp <- sum(map_dbl(all_results, "tp"))
total_fp <- sum(map_dbl(all_results, "fp"))
total_fn <- sum(map_dbl(all_results, "fn"))
total_splits <- sum(map_dbl(all_results, "splits"))
total_merges <- sum(map_dbl(all_results, "merges"))
all_matched_ious <- unlist(map(all_results, "matched_ious"))

# --- 6. Calculate and Print Final Metrics ---

precision <- if (total_tp + total_fp > 0) total_tp / (total_tp + total_fp) else 0
recall    <- if (total_tp + total_fn > 0) total_tp / (total_tp + total_fn) else 0
f1_score  <- if (precision + recall > 0) 2 * (precision * recall) / (precision + recall) else 0
mean_iou  <- if (length(all_matched_ious) > 0) mean(all_matched_ious) else 0

cat("\n--- Segmentation Accuracy Report (v2) ---\n")
cat(sprintf("IoU Threshold: %.2f\n", IOU_THRESHOLD))
cat("------------------------------------\n")
cat(sprintf("Total True Positives (TP):   %d\n", total_tp))
cat(sprintf("Total False Positives (FP):  %d\n", total_fp))
cat(sprintf("Total False Negatives (FN):  %d\n", total_fn))
cat("------------------------------------\n")
cat(sprintf("Total Split Errors (diagnostic): %d\n", total_splits))
cat(sprintf("Total Merge Errors (diagnostic): %d\n", total_merges))
cat("------------------------------------\n")
cat(sprintf("Overall Precision:           %.4f\n", precision))
cat(sprintf("Overall Recall:              %.4f\n", recall))
cat(sprintf("Overall F1-Score:            %.4f\n", f1_score))
cat("------------------------------------\n")
cat(sprintf("Mean IoU of Matched Objects: %.4f\n", mean_iou))
cat("------------------------------------\n")

"/share/lab_crd/lab_crd/HighPloidy_CostBenefits/data/GlucoseStarvation/data/train/cellpose_inputs"
"/share/lab_crd/lab_crd/HighPloidy_CostBenefits/data/GlucoseStarvation/data/train/models/cpsam-tuned"
"/share/lab_crd/lab_crd/HighPloidy_CostBenefits/data/GlucoseStarvation/data/train/cellpose_inputs/tuned-cpsam-masks"
```


python -m cellpose \
  --dir data/train/cellpose_inputs \
  --pretrained_model data/train/models/cpsam-tuned \
  --save_tif \
  --output data/train/tuned-cpsam-masks \
  --verbose \
  --use_gpu \
  --batch_size 64
  
```{r}
library(dplyr)
library(tidyr)
library(jsonlite)

# Load and process manual point labels
manual_df <- fromJSON("data/eval/labelling_output_points/session_point_labels.json") %>%
  filter(label %in% c("alive", "dead")) %>%
  count(image, label) %>%
  pivot_wider(
    names_from = label,
    values_from = n,
    names_glue = "{label}_manual",
    values_fill = 0
  )

# Load and process batch classifier predictions
pred_df <- fromJSON("data/eval/labelling_output_points/batch_predictions.json") %>%
  imap_dfr(~ tibble(
    alive_pred = sum(unlist(.x) == 1),
    dead_pred = sum(unlist(.x) == 2)
  ), .id = "image")

# Join the two datasets
comparison_df <- full_join(manual_df, pred_df, by = "image") %>%
  mutate(across(where(is.numeric), ~ replace_na(., 0))) %>%
  select(image, ends_with("_manual"), ends_with("_pred"))

meta <- do.call(rbind,pbapply::pblapply(comparison_df$image,get_meta))

df <- cbind(comparison_df,meta)

p <- ggplot(df)+
  facet_wrap(~cellLine)+
  geom_abline()+
  geom_point(aes(x=alive_manual,y=alive_pred,color="alive"))+
  geom_point(aes(x=dead_manual,y=dead_pred,color="dead"))
p
```
Glucose data calibration. There's something "amiss" with three of the data points in the calibration assay (Didem appears to have excluded three points). Excluding them does result in greater confidence in glucose prediction based on the fluorescence - but is that confidence misplaced? More investigation is warranted.


```{r}

costf <- function(pars,x){
  a <- pars[1]
  b <- pars[2]
  pars <- abs(pars)
  lval <- log(x$Lum)
  lpred <- log(x$G*a+b)
  err <- sum((lval-lpred)^2)
  if(is.na(err)) return(10^12)
  return(err)
}

glu_prob <- function(glu,fluor,a,b,sdest){
  Efluor <- a*glu + b
  dnorm(log(fluor)-log(Efluor),sd=sdest)
}

predf <- function(pars,x=x){
    a <- pars[1]
    b <- pars[2]
    pars <- abs(pars)
    x$G*a+b
}

x <- data.table::fread("data/glucose/processed/SUM-159-chem.SNU668/calibration.csv")
x <- data.table::fread("data/glucose/processed/MCF10A.MDA-MB-231/calibration.csv")
xsd <- aggregate(list(sd=x$Lum),by=list(G=x$G),sd)


fit0 <- lm(Lum~G,data=x)
par0 <- c(log(min(x$Lum)),as.numeric(coef(fit0)["G"]))
fit <- optim(par0,costf,x=x)

x$pLum <- predf(fit$par,x=x)
x$pLumLin <- predict(fit0)

sdlog <- sd(log(x$pLum)-log(x$Lum))

## heteroscedacity 
p <- ggplot(xsd,aes(x=G,y=sd))+
  geom_point()
p


p <- ggplot(x,aes(x=G,y=Lum))+
  geom_point()+
  geom_line(aes(y=pLum,color="multiplicative"))+
  geom_line(aes(y=pLumLin,color="linear"))+
  scale_x_log10()+
  scale_y_log10()
p

xmpl <- data.frame(G=exp(seq(-15,0,by=0.1)),lum=7000)
xmpl$p <- glu_prob(xmpl$G,xmpl$lum,a=fit$par[2],b=fit$par[1],sdest=sdlog)

p <- ggplot(xmpl,aes(x=G,y=p))+
  geom_line()
p


```


Matching glucose and cell line data... and bundling into a nice fitting object.

The glu_map tells which glucose assay data corresponds to which cellcounts data. It is intended that
for each cellLine x experiment combo, there should be a corresponding folder containing the glucose assay data...

```{r}
glu_map <- c('MCF10A-ctrl.A00-IncucyteRawDataLiveDead-varyGlucose-241015'=NULL,
             'MCF10A-hras.A00-IncucyteRawDataLiveDead-varyGlucose-241015'=NULL,
             'MCF10A.A00b-IncucyteRawDataLiveDead-varyGlucose-241015'    ="MCF10A.MDA-MB-231",
             'MCF10A.A00c-IncucyteRawDataLiveDead-varyGlucose-241015'    ="MCF10A.MDA-MB-231",
             'MDA-MB-231.B00-IncucyteRawDataLiveDead-varyGlucose-250213' ="MCF10A.MDA-MB-231",
             'SNU668.A00-IncucyteRawDataLiveDead-varyGlucose-250324'     ="SUM-159-chem.SNU668",
             'SUM-159-chem.M00b-IncucyteRawDataLiveDead-varyGlucose' = "SUM-159-chem.SNU668",   
             'SUM-159-fuse.C00-IncucyteRawDataLiveDead-varyGlucose'      =NULL,
             'SUM-159-fuse.I00-IncucyteRawDataLiveDead-varyGlucose' ="SUM-159-fuse")
```


```{r}
ploidy_map <- list('SUM-159-chem'=c("2N"="low","4N"="high"),
                   SNU668=c(high="high",low="low"),
                   MCF10A=c("4N"="high","2N"="low"),
                   'MDA-MB-231'=c(parental="high","3N"="low"),
                   'SUM-159-fuse'=c("2N"="low","4N"="high"))
```


```{r}

costf <- function(pars,x){
  a <- pars[1]
  b <- pars[2]
  pars <- abs(pars)
  lval <- log(x$Lum)
  lpred <- log(x$G*a+b)
  err <- sum((lval-lpred)^2)
  if(is.na(err)) return(10^12)
  return(err)
}

predf <- function(pars,x){
    a <- pars[1]
    b <- pars[2]
    pars <- abs(pars)
    x$G*a+b
}

make_glucose_model <- function(xg, pars){
  a <- unname(pars["a"]); b <- unname(pars["b"]); sdlog <- unname(pars["sdlog"])
  stopifnot(is.finite(a), is.finite(b), is.finite(sdlog), sdlog > 0)
  get_pLum <- function(G, dat){
    dilution <- 1000/unique(dat$`Dilution Factor`)
    mu <- a*G*dilution + b                       # expected fluorescence at the provided G
    p  <- dnorm(log(dat$lum) - log(mu), sd = sdlog)  # P(obs lum | G)
    return(p)
  }

  return(get_pLum)
}


process_glucose_assay <- function(path2data){
  x <- data.table::fread(file.path(path2data,"calibration.csv"))
  fit0 <- lm(Lum~G,data=x)
  par0 <- c(log(min(x$Lum)),as.numeric(coef(fit0)["G"]))
  fit <- optim(par0,costf,x=x)
  pars <- fit$par
  names(pars) <- c("a","b")
  x$pLum <- predf(pars,x=x)
  sdlog <- sd(log(x$pLum)-log(x$Lum))
  pars <- c(pars,sdlog=sdlog)
  
  xg <- data.table::fread(file.path(path2data,"data.csv"))
  tmp <- xg[is.na(xg$ploidy),]
  xg$ploidy[is.na(xg$ploidy)] <- "low"
  tmp$ploidy[is.na(tmp$ploidy)] <- "high"
  xg <- rbind(xg,tmp)  
  xg <- reshape2::melt(xg,measure.vars = paste0("R",1:3))
  xg$hours <- xg$Day*24
  xg <- xg[,c("CellLine","ploidy", "G0", "hours", "value","Dilution Factor")]
  colnames(xg)[colnames(xg)=="value"] <- "lum"
  colnames(xg)[colnames(xg)=="CellLine"] <- "cellLine"
  
  list(glucose=xg,ll_lum=make_glucose_model(xg, pars))
}

ff <- list.files("data/glucose/processed",full.names = T)

for(path2data in ff){
  x <- process_glucose_assay(path2data)
  saveRDS(x,file.path(path2data,"fit_obj.Rds"))
  test_dat <- x$glucose%>%filter(cellLine==x$glucose$cellLine[1],hours==0,G0==5)
  test_gluc <- seq(0,10,0.1)
  test_prob <- sapply(test_gluc,function(gi){
    prod(x$ll_lum(gi,test_dat))
  })
  
  plot(test_gluc,test_prob)
}






```
```{r}

x <- readRDS("data/counts/corrected.Rds")
x <- x[paste0(x$cellLine,".",x$experiment)%in%names(glu_map),] ## ultimately need not be necessary?
x <- split(x,f=paste0(x$cellLine,".",x$experiment,".",x$ploidy))

x <- lapply(x,function(xi){
  
  xi <- rename(xi,G0=glucose)
  cellLine <- xi$cellLine[1]
  xi$ploidy <- ploidy_map[[cellLine]][xi$ploidy]
  ploidy <- xi$ploidy[1]
  experiment <- xi$experiment[1]
  cells <- xi[,c("cellLine","ploidy","G0","hours","N","D")]

  glucose_id <- paste(c(cellLine,experiment),collapse=".")
  data <- readRDS(file.path("data/glucose/processed/",glu_map[glucose_id],"fit_obj.Rds"))
  data$glucose <- data$glucose[data$glucose$cellLine==cellLine& data$glucose$ploidy==ploidy,]
  data$glucose <- data$glucose[,c("G0", "hours", "lum", "Dilution Factor")]
  data$cells <- cells[,c("G0","hours","N","D")]
  data$id <- paste(c(cellLine,ploidy,experiment),collapse=".")
  
  saveRDS(data,paste0("data/model_inputs/",data$id,".Rds"))
  return(0)
})



```

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)

plot_cell_line_summary <- function(cell_line, data_dir = "data/model_inputs") {
  
  # 1. Find all files matching the cell line pattern
  pattern <- paste0("^", cell_line, "\\.") 
  files <- list.files(data_dir, pattern = pattern, full.names = TRUE)
  
  if(length(files) == 0) stop(paste("No files found for cell line:", cell_line))
  
  # 2. Iterate, Load, Process
  processed_list <- lapply(files, function(f) {
    
    dat <- readRDS(f)
    f_name <- basename(f)
    # Extract ploidy (2nd element in dot-separated filename)
    ploidy_label <- strsplit(f_name, "\\.")[[1]][2] 
    
    # --- Extract Params for THIS specific experiment ---
    env <- environment(dat$ll_lum)
    a_est <- env$a
    b_est <- env$b
    
    # --- Process Glucose ---
    df_gluc <- dat$glucose %>%
      mutate(
        ploidy = ploidy_label,
        G0 = as.numeric(G0), # Ensure numeric for correct sorting
        dilution_factor = 1000 / `Dilution Factor`,
        G_est = (lum - b_est) / (a_est * dilution_factor),
        G_est = pmax(0, G_est)
      ) %>%
      select(G0, hours, ploidy, G_est)
    
    # --- Process Cells (Calculate Alive = Total - Dead) ---
    df_cells <- dat$cells %>%
      mutate(
        ploidy = ploidy_label,
        G0 = as.numeric(G0), # Ensure numeric for correct sorting
        A = N - D  # Calculate Alive cells
      ) %>% 
      select(G0, hours, ploidy, A, D) # Keep only Alive (A) and Dead (D)
    
    return(list(gluc = df_gluc, cells = df_cells))
  })
  
  # 3. Merge all loaded data
  all_glucose <- bind_rows(lapply(processed_list, `[[`, "gluc"))
  all_cells   <- bind_rows(lapply(processed_list, `[[`, "cells"))
  
  # 4. Summarize for Plotting
  
  # Summarize Cells (Now A and D)
  summ_cells <- all_cells %>%
    pivot_longer(cols = c(A, D), names_to = "State", values_to = "Count") %>%
    mutate(State = factor(State, levels = c("A", "D"), labels = c("Alive", "Dead"))) %>%
    group_by(G0, hours, ploidy, State) %>%
    summarise(
      mean_val = mean(Count, na.rm=TRUE),
      sd_val = sd(Count, na.rm=TRUE),
      .groups = "drop"
    )
  
  # Summarize Glucose
  summ_gluc <- all_glucose %>%
    group_by(G0, hours, ploidy) %>%
    summarise(
      mean_val = mean(G_est, na.rm=TRUE),
      sd_val = sd(G_est, na.rm=TRUE),
      .groups = "drop"
    )

  # 5. Generate Plots
  
  # Plot 1: Cell Counts (Alive vs Dead)
  p_cells <- ggplot(summ_cells, aes(x = hours, y = mean_val, color = State, group = State)) +
    geom_errorbar(aes(ymin = pmax(0, mean_val - sd_val), 
                      ymax = mean_val + sd_val), width = 4, alpha = 0.5) +
    geom_line() +
    geom_point(size = 1.5) +
    # Use as.factor(G0) in facet if you want neat labels, but numeric underlying data ensures sort order
    facet_grid(rows = vars(G0), cols = vars(ploidy), labeller = label_both,scales="free") +
    scale_color_manual(values = c("Alive" = "#2E8B57", "Dead" = "#CD5C5C")) +
    labs(title = paste(cell_line, "- Cell Counts (Alive vs Dead)"),
         y = "Count", x = "Time (Hours)") +
    theme_bw() +
    theme(legend.position = "bottom")
  
  # Plot 2: Glucose Estimates
  p_gluc <- ggplot(summ_gluc, aes(x = hours, y = mean_val)) +
    geom_hline(aes(yintercept = G0), linetype = "dashed", color = "gray60") +
    geom_errorbar(aes(ymin = pmax(0, mean_val - sd_val), 
                      ymax = mean_val + sd_val), width = 4, color = "blue", alpha = 0.5) +
    geom_line(color = "blue") +
    geom_point(color = "blue", size = 1.5) +
    facet_grid(rows = vars(G0), cols = vars(ploidy), labeller = label_both,scales="free") +
    labs(title = paste(cell_line, "- Estimated Glucose"),
         y = "[Glucose] (mM)", x = "Time (Hours)") +
    theme_bw()
  
  return(list(cells = p_cells, glucose = p_gluc))
}
lines <- c("MCF10A","MDA-MB-231","SNU668","SUM-159-chem","SUM-159-fuse")

plots <- lapply(lines,function(li){
  plot_cell_line_summary(li)
})
```



```{r}



x <- readRDS("data/counts/corrected.Rds")
x <- split(x,f=paste0(x$cellLine,".",x$experiment))

xi <- x[[6]]
id <- names(x)[6]
data <- readRDS(file.path("data/glucose/processed/",glu_map[id],"fit_obj.Rds"))

xi <- rename(xi,G0=glucose)
data$cells <- xi[,c("cellLine","ploidy","G0","hours","N","D")]

data$cells <- filter(data$cells,ploidy=="high",cellLine=="SNU668")[,c("G0","hours","N","D")]
data$glucose <- filter(data$glucose,ploidy=="high",cellLine=="SNU668")[,c("G0","hours","lum","Dilution Factor")]

par_info <- data.table(
  par   = c("theta", "kp", "kd", "kd2", "g50a", "na", "g50d", "nd", "v1", "v2"),
  lower = c(1e3, 1e-3, 1e-5, 1e-5, 1e-5, 1, 1e-5, 1, 1e-8, 1e-8),
  upper = c(1e5, 1, 10, 10, 5, 10, 1, 10, 1e-3, 1e-3)
)
source("scripts/rxode_models.R")
source("scripts/parameter_estimation_tmp.R")

model <- load_model("model_B")

tstvals <- pbapply::pbsapply(1:200,function(i){
  logp <- runif(length(par_bounds$par),log(par_bounds$lower),log(par_bounds$upper))
  deoptim_objective_function(logp,model,data,par_names=par_bounds$par)
})



```


```{r}

y <- data.table::fread("data/glucose/processed/250925_Glucose_Promega_chemicalSUM159-SNU668_Andor.csv")

tmp <- y[is.na(y$ploidy),]
y$ploidy[is.na(y$ploidy)]<-"2N"
tmp$ploidy <- "4N"
y <- rbind(tmp,y)

pg <- ggplot(y,aes(x=Day,y=G,color=ploidy,group=ploidy))+
  facet_grid(cols=vars(CellLine),rows=vars(G0),scales="free")+
  geom_point()+
  geom_line()+
  scale_y_continuous("Glucose (mM)")
pg

```
```{r}
x <- readRDS("data/counts/corrected.Rds")

xx <- x[x$cellLine%in%c("SNU668","SUM-159-chem"),]
xx <- aggregate(list(N=xx$N,D=xx$D),by=list(ploidy=xx$ploidy,
                                            glucose=xx$glucose,
                                            hours=xx$hours,
                                            cellLine=xx$cellLine),
                mean)


pn <- ggplot(xx,aes(x=hours/24,y=N,color=ploidy,group=ploidy))+
  facet_grid(rows=vars(as.numeric(glucose)),cols=vars(cellLine),scales="free")+
  geom_point()+
  scale_x_continuous("days")+
  scale_y_continuous("alive cells (per field)")
pn


```

